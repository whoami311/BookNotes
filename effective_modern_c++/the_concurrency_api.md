# 并发 API

C++11 的伟大成功之一是将并发整合到语言和库中。熟悉其他线程 API（比如 pthreads 或者 Windows threads）的开发者有时可能会对 C++ 提供的斯巴达式（译者注：应该是简陋和严谨的意思）功能集感到惊讶，这是因为 C++ 对于并发的大量支持是在对编译器作者约束的层面。由此产生的语言保证意味着在 C++ 的历史中，开发者首次通过标准库可以写出跨平台的多线程程序。这为构建表达库奠定了坚实的基础，标准库并发组件（任务 *tasks*，期望 *futures*，线程 *threads*，互斥 *mutexes*，条件变量 *condition variables*，原子对象 *atomic objects* 等）仅仅是成为并发软件开发者丰富工具集的基础。

在接下来的条款中，记住标准库有两个future的模板：`std::future` 和 `std::shared_future`。在许多情况下，区别不重要，所以我们经常简单的混于一谈为 *futures*。

## Item 35：优先考虑基于任务的编程而非基于线程的编程

如果开发者想要异步执行函数，通常有两种方式。其一是通过创建 `std::thread` 执行，这是应用了**基于线程**（*thread-based*）的方式。

其二是将函数传递给 `std::async`，一种**基于任务**（*task-based*）的策略。这种方式中，传递给 `std::async` 的函数对象被称为一个**任务**（task）。

基于任务的方法通常比基于线程的方法更优，原因之一是基于任务的方法代码量更少。假设调用函数的代码对于其提供的返回值是有需求的。基于线程的方法对此无能为力，而基于任务的方法就简单了，因为 `std::async` 返回的 future 提供了 `get` 函数（从而可以获取返回值）。如果执行函数发生了异常，`get` 函数就显得更为重要，因为 `get` 函数可以提供抛出异常的访问，而基于线程的方法，如果执行函数抛出了异常，程序会直接终止（通过调用 `std::terminate`）。

基于线程与基于任务最根本的区别在于，基于任务的抽象层次更高。基于任务的方式使得开发者从线程管理的细节中解放出来，对此在 C++ 并发软件中总结了“thread”的三种含义：

- **硬件线程**（hardware threads）是真实执行计算的线程。现代计算机体系结构为每个 CPU 核心提供一个或者多个硬件线程。
- **软件线程**（software threads）（也被称为系统线程（OS threads、system threads））是操作系统（假设有一个操作系统。有些嵌入式系统没有。）管理的在硬件线程上执行的线程。通常可以存在比硬件线程更多数量的软件线程，因为当软件线程被阻塞的时候（比如 I/O、同步锁或者条件变量），操作系统可以调度其他未阻塞的软件线程执行提供吞吐量。
- `std::thread` 是 C++ 执行过程的对象，并作为软件线程的句柄（handle）。有些 `std::thread` 对象代表“空”句柄，即没有对应软件线程，因为它们处在默认构造状态（即没有函数要执行）；有些被移动走（移动到的 `std::thread` 就作为这个软件线程的句柄）；有些被 `join`（它们要运行的函数已经运行完）；有些被 `detach`（它们和对应的软件线程之间的连接关系被打断）。

软件线程是有限的资源。如果开发者试图创建大于系统支持的线程数量，会抛出 `std::system_error` 异常。

设计良好的软件必须能有效地处理这种可能性，但是怎样做？一种方法是在当前线程执行 `doAsyncWork`，但是这可能会导致负载不均，而且如果当前线程是 GUI 线程，可能会导致响应时间过长的问题。另一种方法是等待某些当前运行的软件线程结束之后再创建新的 `std::thread`，但是仍然有可能当前运行的线程在等待 `doAsyncWork` 的动作（例如产生一个结果或者报告一个条件变量）。

即使没有超出软件线程的限额，仍然可能会遇到**资源超额**（*oversubscription*）的麻烦。这是一种当前准备运行的（即未阻塞的）软件线程大于硬件线程的数量的情况。情况发生时，线程调度器（操作系统的典型部分）会将软件线程时间切片，分配到硬件上。当一个软件线程的时间片执行结束，会让给另一个软件线程，此时发生上下文切换。软件线程的上下文切换会增加系统的软件线程管理开销，当软件线程安排到与上次时间片运行时不同的硬件线程上，这个开销会更高。这种情况下，（1）CPU缓存对这个软件线程很冷淡（即几乎没有什么数据，也没有有用的操作指南）；（2）“新”软件线程的缓存数据会“污染”“旧”线程的数据，旧线程之前运行在这个核心上，而且还有可能再次在这里运行。

避免资源超额很困难，因为软件线程之于硬件线程的最佳比例取决于软件线程的执行频率，那是动态改变的，比如一个程序从 IO 密集型变成计算密集型，执行频率是会改变的。而且比例还依赖上下文切换的开销以及软件线程对于 CPU 缓存的使用效率。此外，硬件线程的数量和 CPU 缓存的细节（比如缓存多大，相应速度多少）取决于机器的体系结构，即使经过调校，在某一种机器平台避免了资源超额（而仍然保持硬件的繁忙状态），换一个其他类型的机器这个调校并不能提供较好效果的保证。

如果你把这些问题推给另一个人做，你就会变得很轻松，而使用 `std::async` 就做了这件事。

这种调用方式将线程管理的职责转交给 C++ 标准库的开发者。举个例子，这种调用方式会减少抛出资源超额异常的可能性，因为这个调用可能不会开启一个新的线程。你会想：“怎么可能？如果我要求比系统可以提供的更多的软件线程，创建 `std::thread` 和调用 `std::async` 为什么会有区别？”确实有区别，因为以这种形式调用（即使用默认启动策略——见 Item 36）时，`std::async` 不保证会创建新的软件线程。然而，他们允许通过调度器来将特定函数（本例中为 `doAsyncWork`）运行在等待此函数结果的线程上（即在对返回值调用 `get` 或者 `wait` 的线程上），合理的调度器在系统资源超额或者线程耗尽时就会利用这个自由度。

如果考虑自己实现“在等待结果的线程上运行输出结果的函数”，之前提到了可能引出负载不均衡的问题，这问题不那么容易解决，因为应该是 `std::async` 和运行时的调度程序来解决这个问题而不是你。遇到负载不均衡问题时，对机器内发生的事情，运行时调度程序比你有更全面的了解，因为它管理的是所有执行过程，而不仅仅个别开发者运行的代码。

有了 `std::async`，GUI 线程中响应变慢仍然是个问题，因为调度器并不知道你的哪个线程有高响应要求。这种情况下，你会想通过向 `std::async` 传递 `std::launch::async` 启动策略来保证想运行函数在不同的线程上执行（见 Item 36）。

最前沿的线程调度器使用系统级线程池（*thread pool*）来避免资源超额的问题，并且通过工作窃取算法（*work-stealing algorithm*）来提升了跨硬件核心的负载均衡。C++ 标准实际上并不要求使用线程池或者工作窃取，实际上 C++11 并发规范的某些技术层面使得实现这些技术的难度可能比想象中更有挑战。不过，库开发者在标准库实现中采用了这些技术，也有理由期待这个领域会有更多进展。如果你当前的并发编程采用基于任务的方式，在这些技术发展中你会持续获得回报。相反如果你直接使用 `std::thread` 编程，处理线程耗尽、资源超额、负载均衡问题的责任就压在了你身上，更不用说你对这些问题的解决方法与同机器上其他程序采用的解决方案配合得好不好了。

对比基于线程的编程方式，基于任务的设计为开发者避免了手动线程管理的痛苦，并且自然提供了一种获取异步执行程序的结果（即返回值或者异常）的方式。当然，仍然存在一些场景直接使用 `std::thread` 会更有优势：

- **你需要访问非常基础的线程 API**。C++ 并发 API 通常是通过操作系统提供的系统级 API（pthreads 或者 Windows threads）来实现的，系统级 API 通常会提供更加灵活的操作方式（举个例子，C++ 没有线程优先级和亲和性的概念）。为了提供对底层系统级线程 API 的访问，`std::thread` 对象提供了 `native_handle` 的成员函数，而 `std::future`（即 `std::async` 返回的东西）没有这种能力。
- **你需要且能够优化应用的线程使用**。举个例子，你要开发一款已知执行概况的服务器软件，部署在有固定硬件特性的机器上，作为唯一的关键进程。
- **你需要实现 C++ 并发 API 之外的线程技术**，比如，C++ 实现中未支持的平台的线程池。

这些都是在应用开发中并不常见的例子，大多数情况，开发者应该优先采用基于任务的编程方式。

**请记住：**

- `std::thread` API 不能直接访问异步执行的结果，如果执行函数有异常抛出，代码会终止执行。
- 基于线程的编程方式需要手动的线程耗尽、资源超额、负责均衡、平台适配性管理。
- 通过带有默认启动策略的 `std::async` 进行基于任务的编程方式会解决大部分问题。

## Item 36：如果有异步的必要请指定 `std::launch::async`

当你调用 `std::async` 执行函数时（或者其他可调用对象），你通常希望异步执行函数。但是这并不一定是你要求 `std::async` 执行的操作。你事实上要求这个函数按照 `std::async` 启动策略来执行。有两种标准策略，每种都通过 `std::launch` 这个限域 `enum` 的一个枚举名表示（关于枚举的更多细节参见 Item 10）。假定一个函数 `f` 传给 `std::async` 来执行：

- `std::launch::async` **启动策略**意味着 `f` 必须异步执行，即在不同的线程。
- `std::launch::deferred` **启动策略**意味着 `f` 仅当在 `std::async` 返回的 `future` 上调用 `get` 或者 `wait` 时才执行。这表示 `f` **推迟**到存在这样的调用时才执行（译者注：异步与并发是两个不同概念，这里侧重于惰性求值）。当 `get` 或 `wait` 被调用，`f` 会同步执行，即调用方被阻塞，直到 `f` 运行结束。如果 `get` 和 `wait` 都没有被调用，`f` 将不会被执行。（这是个简化说法。关键点不是要在其上调用 `get` 或 `wait` 的那个 `future`，而是 `future` 引用的那个共享状态。（Item 38 讨论了 `future` 与共享状态的关系。）因为 `std::future` 支持移动，也可以用来构造 `std::shared_future`，并且因为 `std::shared_future` 可以被拷贝，对共享状态——对 `f` 传到的那个 `std::async` 进行调用产生的——进行引用的 `future` 对象，有可能与 `std::async` 返回的那个 `future` 对象不同。这非常绕口，所以经常回避这个事实，简称为在 `std::async` 返回的 `future` 上调用 `get` 或 `wait`。）

可能让人惊奇的是，`std::async` 的默认启动策略——你不显式指定一个策略时它使用的那个——不是上面中任意一个。相反，是求或在一起的。下面的两种调用含义相同：

```c++
auto fut1 = std::async(f);                      //使用默认启动策略运行f
auto fut2 = std::async(std::launch::async |     //使用async或者deferred运行f
                       std::launch::deferred,
                       f);
```

因此默认策略允许 `f` 异步或者同步执行。如同 Item 35 中指出，这种灵活性允许 `std::async` 和标准库的线程管理组件承担线程创建和销毁的责任，避免资源超额，以及平衡负载。这就是使用 `std::async` 并发编程如此方便的原因。

但是，使用默认启动策略的 `std::async` 也有一些有趣的影响。给定一个线程 `t` 执行此语句：

```c++
auto fut = std::async(f);   //使用默认启动策略运行f
```

- **无法预测 `f` 是否会与 `t` 并发运行**，因为 `f` 可能被安排延迟运行。
- **无法预测 `f` 是否会在与某线程相异的另一线程上执行，这个某线程在 `fut` 上调用 `get` 或 `wait`**。如果对 `fut` 调用函数的线程是 `t`，含义就是无法预测 `f` 是否在异于 `t` 的另一线程上执行。
- **无法预测 `f` 是否执行**，因为不能确保在程序每条路径上，都会不会在 `fut` 上调用 `get` 或者 `wait`。

默认启动策略的调度灵活性导致使用 thread_local 变量比较麻烦，因为这意味着如果 `f` 读写了**线程本地存储**（thread-local storage，TLS），不可能预测到哪个线程的变量被访问：

```c++
auto fut = std::async(f);   //f的TLS可能是为单独的线程建的，
                            //也可能是为在fut上调用get或者wait的线程建的
```

这还会影响到基于 `wait` 的循环使用超时机制，因为在一个延时的任务（参见 Item 35）上调用 `wait_for` 或者 `wait_until` 会产生 `std::launch::deferred` 值。意味着，以下循环看似应该最终会终止，但可能实际上永远运行：

```c++
using namespace std::literals;      //为了使用C++14中的时间段后缀；参见条款34

void f()                            //f休眠1秒，然后返回
{
    std::this_thread::sleep_for(1s);
}

auto fut = std::async(f);           //异步运行f（理论上）

while (fut.wait_for(100ms) !=       //循环，直到f完成运行时停止...
       std::future_status::ready)   //但是有可能永远不会发生！
{
    …
}
```

这种错误很容易在开发和单元测试中忽略，因为它可能在负载过高时才能显现出来。那些是使机器资源超额或者线程耗尽的条件，此时任务推迟执行才最有可能发生。毕竟，如果硬件没有资源耗尽，没有理由不安排任务并发执行。

修复也是很简单的：只需要检查与 `std::async` 对应的 `future` 是否被延迟执行即可，那样就会避免进入无限循环。不幸的是，没有直接的方法来查看 `future` 是否被延迟执行。相反，你必须调用一个超时函数——比如 `wait_for` 这种函数。在这个情况中，你不想等待任何事，只想查看返回值是否是 `std::future_status::deferred`，所以无须怀疑，使用 `0` 调用 `wait_for`：

```c++
auto fut = std::async(f);               //同上

if (fut.wait_for(0s) ==                 //如果task是deferred（被延迟）状态
    std::future_status::deferred)
{
    …                                   //在fut上调用wait或get来异步调用f
} else {                                //task没有deferred（被延迟）
    while (fut.wait_for(100ms) !=       //不可能无限循环（假设f完成）
           std::future_status::ready) {
        …                               //task没deferred（被延迟），也没ready（已准备）
                                        //做并行工作直到已准备
    }
    …                                   //fut是ready（已准备）状态
}
```

这些各种考虑的结果就是，只要满足以下条件，`std::async` 的默认启动策略就可以使用：

- 任务不需要和执行 `get` 或 `wait` 的线程并行执行。
- 读写哪个线程的 thread_local 变量没什么问题。
- 可以保证会在 `std::async` 返回的 `future` 上调用 `get` 或 `wait`，或者该任务可能永远不会执行也可以接受。
- 使用 `wait_for` 或 `wait_until` 编码时考虑到了延迟状态。

如果上述条件任何一个都满足不了，你可能想要保证 `std::async` 会安排任务进行真正的异步执行。进行此操作的方法是调用时，将 `std::launch::async` 作为第一个实参传递：

```c++
auto fut = std::async(std::launch::async, f);   //异步启动f的执行
```

事实上，对于一个类似 `std::async` 行为的函数，但是会自动使用 `std::launch::async` 作为启动策略的工具，拥有它会非常方便，而且编写起来很容易也使它看起来很棒。C++11 版本如下：

```c++
template<typename F, typename... Ts>
inline
std::future<typename std::result_of<F(Ts...)>::type>
reallyAsync(F&& f, Ts&&... params)          //返回异步调用f(params...)得来的future
{
    return std::async(std::launch::async,
                      std::forward<F>(f),
                      std::forward<Ts>(params)...);
}

auto fut = reallyAsync(f); //异步运行f，如果std::async抛出异常它也会抛出
```

在 C++14 中，`reallyAsync` 返回类型的推导能力可以简化函数的声明：

```c++
template<typename F, typename... Ts>
inline
auto                                        // C++14
reallyAsync(F&& f, Ts&&... params)
{
    return std::async(std::launch::async,
                      std::forward<F>(f),
                      std::forward<Ts>(params)...);
}
```

这个版本清楚表明，`reallyAsync` 除了使用 `std::launch::async` 启动策略之外什么也没有做。

**请记住：**

- `std::async` 的默认启动策略是异步和同步执行兼有的。
- 这个灵活性导致访问 thread_locals 的不确定性，隐含了任务可能不会被执行的意思，会影响调用基于超时的 `wait` 的程序逻辑。
- 如果异步执行任务非常关键，则指定 `std::launch::async`。

## Item 37：从各个方面使得 `std::thread` unjoinable

每个 `std::thread` 对象处于两个状态之一：**可结合的**（joinable）或者**不可结合的**（unjoinable）。可结合状态的 `std::thread` 对应于正在运行或者可能要运行的异步执行线程。

不可结合的 `std::thread` 正如所期待：一个不是可结合状态的 `std::thread`。不可结合的 `std::thread` 对象包括：

- **默认构造的**`std::thread`。这种 `std::thread` 没有函数执行，因此没有对应到底层执行线程上。
- **已经被移动走的**`std::thread` 对象。移动的结果就是一个 `std::thread` 原来对应的执行线程现在对应于另一个 `std::thread`。
- **已经被 `join` 的**`std::thread`。在 `join` 之后，`std::thread` 不再对应于已经运行完了的执行线程。
- **已经被 `detach` 的** `std::thread` 。`detach` 断开了 `std::thread` 对象与执行线程之间的连接。

`std::thread` 的可结合性如此重要的原因之一就是当可结合的线程的析构函数被调用，程序执行会终止。

为什么`std::thread` 析构的行为是这样的，那是因为另外两种显而易见的方式更糟：

- **隐式 `join`**。这种情况下，`std::thread` 的析构函数将等待其底层的异步执行线程完成。这听起来是合理的，但是可能会导致难以追踪的异常表现。
- **隐式 `detach`**。这种情况下，`std::thread` 析构函数会分离 `std::thread` 与其底层的线程。底层线程继续运行。听起来比 `join` 的方式好，但是可能导致更严重的调试问题。

标准委员会认为，销毁可结合的线程如此可怕以至于实际上禁止了它（规定销毁可结合的线程导致程序终止）。

这使你有责任确保使用 `std::thread` 对象时，在所有的路径上超出定义所在的作用域时都是不可结合的。

每当你想在执行跳至块之外的每条路径执行某种操作，最通用的方式就是将该操作放入局部对象的析构函数中。这些对象称为 **RAII 对象**（RAII objects），从 **RAII 类**中实例化。（RAII 全称为 “Resource Acquisition Is Initialization”（资源获得即初始化），尽管技术关键点在析构上而不是实例化上）。但是标准库没有 `std::thread` 的 RAII 类，可能是因为标准委员会拒绝将 `join` 和 `detach` 作为默认选项，不知道应该怎么样完成 RAII。

```c++
class ThreadRAII {
public:
    enum class DtorAction { join, detach };     //enum class的信息见条款10
    
    ThreadRAII(std::thread&& t, DtorAction a)   //析构函数中对t实行a动作
    : action(a), t(std::move(t)) {}

    ~ThreadRAII()
    {                                           //可结合性测试见下
        if (t.joinable()) {
            if (action == DtorAction::join) {
                t.join();
            } else {
                t.detach();
            }
        }
    }

    ThreadRAII(ThreadRAII&&) = default;             //支持移动
    ThreadRAII& operator=(ThreadRAII&&) = default;

    std::thread& get() { return t; }            //见下

private:
    DtorAction action;
    std::thread t;
};
```

这种情况下，我们选择在 `ThreadRAII` 的析构函数对异步执行的线程进行 `join`，因为在先前分析中，`detach` 可能导致噩梦般的调试过程。我们之前也分析了 `join` 可能会导致表现异常（坦率说，也可能调试困难），但是在未定义行为（`detach` 导致），程序终止（使用原生 `std::thread` 导致），或者表现异常之间选择一个后果，可能表现异常是最好的那个。

**请记住：**

- 在所有路径上保证 `thread` 最终是不可结合的。
- 析构时 `join` 会导致难以调试的表现异常问题。
- 析构时 `detach` 会导致难以调试的未定义行为。
- 声明类数据成员时，最后声明 `std::thread` 对象。

## Item 38：关注不同线程句柄的析构行为

TODO

## Item 39：

TODO

## Item 40：对于并发使用 `std::atomic`，对于特殊内存使用 `volatile`

可怜的 `volatile`。如此令人迷惑。本不应该出现在本章节，因为它跟并发编程没有关系。但是在其他编程语言中（比如，Java 和 C#），`volatile`是有并发含义的，即使在 C++ 中，有些编译器在实现时也将并发的某种含义加入到了 `volatile` 关键字中（但仅仅是在用那些编译器时）。因此在此值得讨论下关于 `volatile` 关键字的含义以消除异议。

开发者有时会与 `volatile` 混淆的特性——本来应该属于本章的那个特性——是 `std::atomic` 模板。这种模板的实例化提供了一种在其他线程看来操作是原子性的的保证（译注：即某些操作是像原子一样的不可分割。）。一旦 `std::atomic` 对象被构建，在其上的操作表现得像操作是在互斥锁保护的关键区内，但是通常这些操作是使用特定的机器指令实现，这比锁的实现更高效。

读-改-写（read-modify-write，RMW）操作，它们整体作为原子执行。这是 `std::atomic` 类型的最优的特性之一：一旦 `std::atomic` 对象被构建，所有成员函数，包括 RMW 操作，从其他线程来看都是原子性的。

```c++
std::atomic<bool> valVailable(false); 
auto imptValue = computeImportantValue();   //计算值
valAvailable = true;                        //告诉另一个任务，值可用了
```

人类读这份代码，能看到在 `valAvailabl` e赋值之前对 `imptValue` 赋值很关键，但是所有编译器看到的是给相互独立的变量的一对赋值操作。通常来说，编译器会被允许重排这对没有关联的操作。

即使编译器没有重排顺序，底层硬件也可能重排（或者可能使它看起来运行在其他核心上），因为有时这样代码执行更快。

然而，`std::atomic` 会限制这种重排序，并且这样的限制之一是，在源代码中，对 `std::atomic` 变量写之前不会有任何操作（或者操作发生在其他核心上）。（这只在 `std::atomics` 使用**顺序一致性**（sequential consistency）时成立，对于使用在本书中展示的语法的 `std::atomic` 对象，这也是默认的和唯一的一致性模型。C++11 也支持带有更灵活的代码重排规则的一致性模型。这样的**弱**（weak）（亦称**松散的**，relaxed）模型使构建一些软件在某些硬件构架上运行的更快成为可能，但是使用这样的模型产生的软件**更加**难改正、理解、维护。在使用松散原子性的代码中微小的错误很常见，即使专家也会出错，所以应当尽可能坚持顺序一致性。）

这两个问题——不保证操作的原子性以及对代码重排顺序没有足够限制——解释了为什么 `volatile` 在多线程编程中没用，但是没有解释它应该用在哪。简而言之，它是用来告诉编译器，它们处理的内存有不正常的表现。

```c++
int x;

auto y = x;                             //读x
y = x;                                  //再次读x
x = 10;                                 //写x
x = 20;                                 //再次写x
```

可能你会想谁会写这种重复读写的代码（技术上称为**冗余访问**（redundant loads）和**无用存储**（dead stores）），答案是开发者不会直接写——至少我们不希望开发者这样写。但是在编译器拿到看起来合理的代码，执行了模板实例化，内联和一系列重排序优化之后，结果会出现冗余访问和无用存储，所以编译器需要摆脱这样的情况并不少见。

这种优化仅仅在内存表现正常时有效。“特殊”的内存不行。最常见的“特殊”内存是用来做内存映射 I/O 的内存。这种内存实际上是与外围设备（比如外部传感器或者显示器，打印机，网络端口）通信，而不是读写通常的内存（比如 RAM ）。

`volatile` 是告诉编译器我们正在处理特殊内存。意味着告诉编译器“不要对这块内存执行任何优化”。所以如果 `x` 对应于特殊内存，应该声明为 `volatile`。

```c++
volatile int x;

auto y = x;                             //读x
y = x;                                  //再次读x（不会被优化掉）

x = 10;                                 //写x（不会被优化掉）
x = 20;                                 //再次写x
```

突击测试！在最后一段代码中，`y` 是什么类型：`int` 还是 `volatile int`？（`y` 的类型使用 `auto` 类型推导，所以使用 Item 2 中的规则。规则上说非引用非指针类型的声明（就是 `y` 的情况），`const` 和 `volatile` 限定符被拿掉。`y` 的类型因此仅仅是 `int`。这意味着对 `y` 的冗余读取和写入可以被消除。在例子中，编译器必须执行对 `y` 的初始化和赋值两个语句，因为 `x` 是 `volatile` 的，所以第二次对 `x` 的读取可能会产生一个与第一次不同的值。）

在处理特殊内存时，必须保留看似冗余访问和无用存储的事实，顺便说明了为什么 `std::atomic` 不适合这种场景。编译器被允许消除对 `std::atomic` 的冗余操作。

```c++
std::atomic<int> x;
auto y = x;                             //错误
y = x;                                  //错误
```

这是因为 `std::atomic` 类型的拷贝操作是被删除的（参见 Item 11）。因为有个很好的理由删除。想象一下如果 `y` 使用 `x` 来初始化会发生什么。因为 `x` 是 `std::atomic` 类型，`y` 的类型被推导为 `std::atomic`（参见 Item 2）。我之前说了 `std::atomic` 最好的特性之一就是所有成员函数都是原子性的，但是为了使从 `x` 拷贝初始化 `y` 的过程是原子性的，编译器不得不生成代码，把读取 `x` 和写入 `y` 放在一个单独的原子性操作中。硬件通常无法做到这一点，因此 `std::atomic` 不支持拷贝构造。出于同样的原因，拷贝赋值也被删除了，这也是为什么从 `x` 赋值给 `y` 也编译失败。（移动操作在 `std::atomic` 没有显式声明，因此根据 Item 17中描述的规则来看，`std::atomic` 不支持移动构造和移动赋值）。

可以将 `x` 的值传递给 `y`，但是需要使用 `std::atomic` 的 `load`和 `store` 成员函数。`load` 函数原子性地读取，`store` 原子性地写入。

```c++
std::atomic<int> y(x.load());           //读x
y.store(x.load());                      //再次读x
```

这可以编译，读取 `x`（通过 `x.load()`）是与初始化或者存储到 `y` 相独立的函数，这个事实清楚地表明没理由期待上面的任何一个语句会在单独的原子性的操作中整体执行。

- `std::atomic` 用在并发编程中，对访问特殊内存没用。
- `volatile` 用于访问特殊内存，对并发编程没用。

因为 `std::atomic` 和 `volatile` 用于不同的目的，所以可以结合起来使用。

最后一点，一些开发者在即使不必要时也尤其喜欢使用 `std::atomic` 的 `load` 和 `store` 函数，因为这在代码中显式表明了这个变量不“正常”。强调这一事实并非没有道理。因为访问 `std::atomic` 确实会比 non-`std::atomic` 更慢一些，我们也看到了 `std::atomic` 会阻止编译器对代码执行一些特定的，本应被允许的顺序重排。调用 `load` 和 `store` 可以帮助识别潜在的可扩展性瓶颈。从正确性的角度来看，没有看到在一个变量上调用 `store` 来与其他线程进行通信（比如用个 flag 表示数据的可用性）可能意味着该变量在声明时本应使用而没有使用 `std::atomic`。

这更多是习惯问题，但是，一定要知道 `atomic` 和 `volatile` 的巨大不同。

**请记住：**

- `std::atomic` 用于在不使用互斥锁情况下，来使变量被多个线程访问的情况。是用来编写并发程序的一个工具。
- `volatile` 用在读取和写入不应被优化掉的内存上。是用来处理特殊内存的一个工具。
